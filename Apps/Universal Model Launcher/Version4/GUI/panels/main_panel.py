"""
ğŸ¯ Main Panel - Multi-Modal Interface
=====================================
Role: Primary content area with tabbed interface
SOLID: Single responsibility for main content display
"""

from PySide6.QtWidgets import (QWidget, QVBoxLayout, QTabWidget, QTextEdit,
                               QLabel, QHBoxLayout, QPushButton, QFileDialog,
                               QGridLayout, QScrollArea, QFrame)
from PySide6.QtCore import Signal, QTimer, Qt
from PySide6.QtGui import QPixmap, QFont

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.absolute()))

from GUI.theme_manager import ThemeManager


class MainPanel(QWidget):
    """
    ğŸ¯ Main Content Panel
    =====================
    Role: Multi-Modal Interface Hub
    Pattern: Tabbed interface for different AI modes
    """
    
    # Signals for inter-panel communication
    performance_updated = Signal(dict)
    log_message = Signal(str, str)  # message, level
    status_changed = Signal(dict)
    
    def __init__(self):
        super().__init__()
        self.theme_manager = ThemeManager()
        
        # Initialize model discovery system
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent.parent.parent.absolute()))
        from Core.model_discovery import ModelDiscovery
        self.model_discovery = ModelDiscovery()
        
        self._setup_ui()
        self._apply_styles()
        self._connect_signals()
        
        # Start model discovery in background with a timer
        from PySide6.QtCore import QTimer
        self.discovery_timer = QTimer()
        self.discovery_timer.setSingleShot(True)
        self.discovery_timer.timeout.connect(self._discover_models)
        self.discovery_timer.start(1000)  # Start discovery after 1 second
    
    def _setup_ui(self):
        """ğŸ—ï¸ Create main panel layout"""
        layout = QVBoxLayout(self)
        layout.setContentsMargins(8, 8, 8, 8)
        
        # Create tabbed interface
        self.tab_widget = QTabWidget()
        
        # Add tabs
        self.model_browser_tab = self._create_model_browser_tab()
        self.vision_tab = self._create_vision_tab()
        self.audio_tab = self._create_audio_tab()
        self.chat_tab = self._create_chat_tab()
        self.settings_tab = self._create_settings_tab()
        
        self.tab_widget.addTab(self.model_browser_tab, "ğŸ” Model Browser")
        self.tab_widget.addTab(self.vision_tab, "ğŸ–¼ï¸ Vision")
        self.tab_widget.addTab(self.audio_tab, "ğŸ”Š Audio")
        self.tab_widget.addTab(self.chat_tab, "ğŸ§  Chat")
        self.tab_widget.addTab(self.settings_tab, "âš™ï¸ Settings")
        
        layout.addWidget(self.tab_widget)
    
    def _create_model_browser_tab(self):
        """ğŸ” Create model browser interface"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # Header
        header = QLabel("ğŸ” Discovered Models")
        header.setFont(QFont("Consolas", 12, QFont.Bold))
        layout.addWidget(header)
        
        # Search bar
        search_layout = QHBoxLayout()
        self.search_input = QTextEdit()
        self.search_input.setMaximumHeight(30)
        self.search_input.setPlaceholderText("Search models...")
        self.search_button = QPushButton("ğŸ” Search")
        
        search_layout.addWidget(self.search_input, 1)
        search_layout.addWidget(self.search_button)
        layout.addLayout(search_layout)
        
        # Models grid (scrollable)
        scroll_area = QScrollArea()
        scroll_widget = QWidget()
        self.models_grid = QGridLayout(scroll_widget)
        
        # Add placeholder model cards
        self._populate_model_browser()
        
        scroll_area.setWidget(scroll_widget)
        scroll_area.setWidgetResizable(True)
        layout.addWidget(scroll_area, 1)
        
        return widget
    
    def _create_vision_tab(self):
        """ğŸ–¼ï¸ Create vision analysis interface"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # Header
        header = QLabel("ğŸ–¼ï¸ Vision Analysis")
        header.setFont(QFont("Consolas", 12, QFont.Bold))
        layout.addWidget(header)
        
        # Upload area
        upload_frame = QFrame()
        upload_frame.setFrameStyle(QFrame.Box)
        upload_frame.setMinimumHeight(200)
        upload_layout = QVBoxLayout(upload_frame)
        
        self.image_label = QLabel("ğŸ“ Drop image here or click to browse")
        self.image_label.setAlignment(Qt.AlignCenter)
        self.image_label.setStyleSheet("border: 2px dashed #00d4aa; border-radius: 8px; padding: 20px;")
        
        self.browse_button = QPushButton("ğŸ“ Browse Images")
        
        upload_layout.addWidget(self.image_label, 1)
        upload_layout.addWidget(self.browse_button)
        layout.addWidget(upload_frame)
        
        # Analysis results
        results_label = QLabel("ğŸ” Analysis Results:")
        results_label.setFont(QFont("Consolas", 10, QFont.Bold))
        layout.addWidget(results_label)
        
        self.vision_results = QTextEdit()
        self.vision_results.setPlaceholderText("Upload an image to see AI analysis results...")
        layout.addWidget(self.vision_results, 1)
        
        return widget
    
    def _create_audio_tab(self):
        """ğŸ”Š Create audio transcription interface"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # Header
        header = QLabel("ğŸ”Š Audio Transcription")
        header.setFont(QFont("Consolas", 12, QFont.Bold))
        layout.addWidget(header)
        
        # Upload controls
        controls_layout = QHBoxLayout()
        self.audio_browse_button = QPushButton("ğŸ“ Browse Audio")
        self.audio_record_button = QPushButton("ğŸ¤ Record")
        self.audio_play_button = QPushButton("â–¶ï¸ Play")
        self.audio_stop_button = QPushButton("â¹ï¸ Stop")
        
        controls_layout.addWidget(self.audio_browse_button)
        controls_layout.addWidget(self.audio_record_button)
        controls_layout.addWidget(self.audio_play_button)
        controls_layout.addWidget(self.audio_stop_button)
        layout.addLayout(controls_layout)
        
        # Waveform placeholder
        waveform_frame = QFrame()
        waveform_frame.setFrameStyle(QFrame.Box)
        waveform_frame.setMinimumHeight(100)
        waveform_layout = QVBoxLayout(waveform_frame)
        waveform_label = QLabel("ğŸŒŠ Audio Waveform (Placeholder)")
        waveform_label.setAlignment(Qt.AlignCenter)
        waveform_layout.addWidget(waveform_label)
        layout.addWidget(waveform_frame)
        
        # Transcription results
        transcription_label = QLabel("ğŸ“ Transcription:")
        transcription_label.setFont(QFont("Consolas", 10, QFont.Bold))
        layout.addWidget(transcription_label)
        
        self.transcription_results = QTextEdit()
        self.transcription_results.setPlaceholderText("Upload or record audio to see transcription...")
        layout.addWidget(self.transcription_results, 1)
        
        return widget
    
    def _create_chat_tab(self):
        """ğŸ§  Create chat interface"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # Header
        header = QLabel("ğŸ§  AI Chat Interface")
        header.setFont(QFont("Consolas", 12, QFont.Bold))
        layout.addWidget(header)
        
        # Chat history
        self.chat_history = QTextEdit()
        self.chat_history.setReadOnly(True)
        self.chat_history.setPlaceholderText("Start chatting with the AI model...")
        layout.addWidget(self.chat_history, 1)
        
        # Input area
        input_layout = QHBoxLayout()
        self.chat_input = QTextEdit()
        self.chat_input.setMaximumHeight(60)
        self.chat_input.setPlaceholderText("Type your message here...")
        
        self.send_button = QPushButton("ğŸ“¤ Send")
        self.clear_button = QPushButton("ğŸ—‘ï¸ Clear")
        
        input_layout.addWidget(self.chat_input, 1)
        input_layout.addWidget(self.send_button)
        input_layout.addWidget(self.clear_button)
        layout.addLayout(input_layout)
        
        return widget
    
    def _create_settings_tab(self):
        """âš™ï¸ Create settings interface"""
        widget = QWidget()
        layout = QVBoxLayout(widget)
        
        # Header
        header = QLabel("âš™ï¸ Configuration & Settings")
        header.setFont(QFont("Consolas", 12, QFont.Bold))
        layout.addWidget(header)
        
        # Model Paths section
        paths_frame = QFrame()
        paths_frame.setFrameStyle(QFrame.Box)
        paths_layout = QVBoxLayout(paths_frame)
        
        paths_label = QLabel("ğŸ“‚ Model Discovery Paths:")
        paths_label.setFont(QFont("Consolas", 10, QFont.Bold))
        paths_layout.addWidget(paths_label)
        
        # Path management controls
        path_controls = QHBoxLayout()
        self.add_path_button = QPushButton("ğŸ“ Add Directory")
        self.refresh_models_button = QPushButton("ğŸ”„ Refresh Models")
        self.open_config_button = QPushButton("âš™ï¸ Open Config")
        
        path_controls.addWidget(self.add_path_button)
        path_controls.addWidget(self.refresh_models_button)
        path_controls.addWidget(self.open_config_button)
        path_controls.addStretch()
        paths_layout.addLayout(path_controls)
        
        # Current paths display
        self.paths_display = QTextEdit()
        self.paths_display.setMaximumHeight(120)
        self.paths_display.setReadOnly(True)
        self._update_paths_display()
        paths_layout.addWidget(self.paths_display)
        
        layout.addWidget(paths_frame)
        
        # API Endpoints section
        api_frame = QFrame()
        api_frame.setFrameStyle(QFrame.Box)
        api_layout = QVBoxLayout(api_frame)
        
        api_label = QLabel("ğŸŒ API Endpoints:")
        api_label.setFont(QFont("Consolas", 10, QFont.Bold))
        api_layout.addWidget(api_label)
        
        self.api_info = QTextEdit()
        self.api_info.setMaximumHeight(120)
        self.api_info.setReadOnly(True)
        self.api_info.setText("""
ï¿½ INTERNAL MODE: Smart inference for controlled apps
ğŸ“¡ EXTERNAL MODE: Fixed ports for external tools
ğŸŒ POST /smart_inference - Smart model selection
ğŸ“Š WS /ws - Real-time streaming
ğŸ”— OpenAI Compatible endpoints available
        """)
        api_layout.addWidget(self.api_info)
        layout.addWidget(api_frame)
        
        # Performance settings
        perf_frame = QFrame()
        perf_frame.setFrameStyle(QFrame.Box)
        perf_layout = QVBoxLayout(perf_frame)
        
        perf_label = QLabel("âš¡ Performance Settings:")
        perf_label.setFont(QFont("Consolas", 10, QFont.Bold))
        perf_layout.addWidget(perf_label)
        
        self.perf_settings = QTextEdit()
        self.perf_settings.setMaximumHeight(80)
        self.perf_settings.setText("""
ğŸ¯ Smart Model Selection: Input analysis + hardware optimization
âš¡ GPU Acceleration: Auto-detected (CUDA/Metal/OpenCL)
ğŸ”§ Port Range: 8080-8090 (configurable in uml_config.json)
        """)
        perf_layout.addWidget(self.perf_settings)
        layout.addWidget(perf_frame)
        
        # Add stretch
        layout.addStretch()
        
        return widget
    
    def _populate_model_browser(self):
        """ğŸ“‹ Populate model browser with discovered models"""
        # Clear existing models
        for i in reversed(range(self.models_grid.count())):
            child = self.models_grid.itemAt(i).widget()
            if child:
                child.setParent(None)
        
        # Check if models have been discovered
        discovered_models = getattr(self, 'discovered_models', {})
        
        if not discovered_models:
            # Show placeholder message
            placeholder = QLabel("ğŸ” Discovering models... Please wait.")
            placeholder.setAlignment(Qt.AlignCenter)
            placeholder.setStyleSheet("color: #848d97; font-style: italic; padding: 20px;")
            self.models_grid.addWidget(placeholder, 0, 0, 1, 2)
            return
        
        # Populate with discovered models (limit to first 20 for performance)
        max_models_to_show = 20
        model_count = 0
        
        for i, (name, model) in enumerate(discovered_models.items()):
            if model_count >= max_models_to_show:
                break
                
            # Format display name with emoji based on model type
            emoji_map = {
                "text": "ğŸ§ ",
                "vision": "ğŸ–¼ï¸", 
                "audio": "ğŸ”Š",
                "code": "ğŸ’»"
            }
            emoji = emoji_map.get(model.model_type, "ğŸ¤–")
            display_name = f"{emoji} {model.name}"
            
            # Create description
            size_mb = model.size_bytes // (1024 * 1024) if model.size_bytes > 0 else 0
            description_parts = []
            if model.parameters:
                description_parts.append(model.parameters)
            if model.description:
                description_parts.append(model.description)
            if size_mb > 0:
                description_parts.append(f"{size_mb} MB")
            
            description = " â€¢ ".join(description_parts) if description_parts else "AI Model"
            
            card = self._create_model_card(display_name, description, model.backend_type, model.path)
            row = model_count // 2
            col = model_count % 2
            self.models_grid.addWidget(card, row, col)
            model_count += 1
        
        # Add info label if there are more models
        if len(discovered_models) > max_models_to_show:
            info_label = QLabel(f"ğŸ“Š Showing {max_models_to_show} of {len(discovered_models)} models. Use search to filter.")
            info_label.setAlignment(Qt.AlignCenter)
            info_label.setStyleSheet("color: #00d4aa; font-style: italic; padding: 10px;")
            row = (model_count + 1) // 2
            self.models_grid.addWidget(info_label, row, 0, 1, 2)
    
    def _create_model_card(self, name: str, description: str, backend: str, model_path: str = ""):
        """ğŸƒ Create a model card widget with real functionality"""
        card = QFrame()
        card.setFrameStyle(QFrame.Box)
        card.setStyleSheet("""
        QFrame {
            background-color: #21262d;
            border: 1px solid #30363d;
            border-radius: 8px;
            padding: 12px;
        }
        QFrame:hover {
            border-color: #00d4aa;
            background-color: #2d333b;
        }
        """)
        
        layout = QVBoxLayout(card)
        
        # Model name
        name_label = QLabel(name)
        name_label.setFont(QFont("Consolas", 10, QFont.Bold))
        layout.addWidget(name_label)
        
        # Description
        desc_label = QLabel(description)
        desc_label.setStyleSheet("color: #7d8590;")
        desc_label.setWordWrap(True)
        layout.addWidget(desc_label)
        
        # Backend info
        backend_label = QLabel(f"Backend: {backend}")
        backend_label.setStyleSheet("color: #7c3aed; font-size: 8pt;")
        layout.addWidget(backend_label)
        
        # Load button with real functionality
        load_button = QPushButton("ğŸ“¥ Load Model")
        load_button.setStyleSheet(self.theme_manager.get_button_style('primary'))
        
        # Connect to actual model loading
        model_name_clean = name.split(' ', 1)[1]  # Remove emoji
        load_button.clicked.connect(
            lambda: self._request_model_load(model_name_clean, backend)
        )
        
        layout.addWidget(load_button)
        
        return card
    
    def _request_model_load(self, model_name: str, backend: str):
        """ğŸ“¥ Request model loading through main window"""
        config = {
            'model': model_name,
            'backend': backend,
            'gpu_layers': 'auto',
            'context': 4096
        }
        
        # Emit signal to main window for processing
        self.log_message.emit(f"Requesting load: {model_name} ({backend})", "info")
        
        # This would be connected to main window's model loading system
        # For now, just show in logs
        self.log_message.emit(f"Model load request sent: {model_name}", "success")
    
    def _apply_styles(self):
        """ğŸ¨ Apply sci-fi theme styles"""
        self.setStyleSheet(self.theme_manager.get_panel_style())
        
        # Style tab widget
        self.tab_widget.setStyleSheet(self.theme_manager.get_tab_style())
        
        # Style buttons
        for button in [self.search_button, self.browse_button, self.audio_browse_button,
                      self.audio_record_button, self.audio_play_button, self.audio_stop_button,
                      self.send_button, self.clear_button]:
            button.setStyleSheet(self.theme_manager.get_button_style('primary'))
        
        # Style text inputs
        for text_widget in [self.search_input, self.vision_results, self.transcription_results,
                           self.chat_history, self.chat_input, self.api_info, self.perf_settings]:
            text_widget.setStyleSheet(self.theme_manager.get_input_style())
    
    def _connect_signals(self):
        """ğŸ”— Connect UI signals"""
        self.browse_button.clicked.connect(self._browse_image)
        self.audio_browse_button.clicked.connect(self._browse_audio)
        self.send_button.clicked.connect(self._send_chat_message)
        self.clear_button.clicked.connect(self._clear_chat)
        
        # Settings tab signals
        if hasattr(self, 'add_path_button'):
            self.add_path_button.clicked.connect(self._add_model_path)
        if hasattr(self, 'refresh_models_button'):
            self.refresh_models_button.clicked.connect(self.refresh_models)
        if hasattr(self, 'open_config_button'):
            self.open_config_button.clicked.connect(self._open_config_file)
    
    def _browse_image(self):
        """ğŸ“ Browse for image file and analyze with vision model"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "Select Image", "", "Images (*.png *.jpg *.jpeg *.bmp *.gif)"
        )
        if file_path:
            self.image_label.setText(f"ğŸ“ Selected: {Path(file_path).name}")
            self.log_message.emit(f"Image selected: {file_path}", "info")
            
            # Process image with vision model
            self._process_image(file_path)
    
    def _process_image(self, file_path: str):
        """ğŸ–¼ï¸ Process image through vision model system"""
        self.vision_results.clear()
        self.vision_results.append("ğŸ” Analyzing image with optimal vision model...")
        
        # In real implementation, this would:
        # 1. Send to hybrid smart loader with image input
        # 2. Auto-select best vision model (CLIP, BakLLaVA, etc.)
        # 3. Get analysis results
        
        # Simulate processing delay
        QTimer.singleShot(2000, lambda: self._display_vision_results(file_path))
    
    def _display_vision_results(self, file_path: str):
        """ğŸ” Display vision analysis results"""
        # Simulate vision analysis results
        results = f"""
ğŸ” Vision Analysis Complete

ğŸ“ File: {Path(file_path).name}
ğŸ¤– Model: BakLLaVA-7B (auto-selected for image analysis)
ğŸ“Š Confidence: 94.7%

ğŸ“ Description:
The image appears to show [simulated analysis]. The optimal vision model was automatically selected based on image properties and current system resources.

ğŸ·ï¸ Detected Objects:
â€¢ Object 1 (confidence: 92%)
â€¢ Object 2 (confidence: 87%)
â€¢ Object 3 (confidence: 76%)

âš¡ Processing Time: 1.8 seconds
ğŸ§  Memory Used: 2.3 GB
        """
        
        self.vision_results.clear()
        self.vision_results.append(results)
        
        # Update performance
        self.performance_updated.emit({
            'memory_usage': 88,
            'gpu_usage': 92,
            'generation_speed': 35
        })
    
    def _browse_audio(self):
        """ğŸ“ Browse for audio file and transcribe"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "Select Audio", "", "Audio (*.wav *.mp3 *.flac *.ogg)"
        )
        if file_path:
            self.log_message.emit(f"Audio selected: {file_path}", "info")
            
            # Process audio with transcription model
            self._process_audio(file_path)
    
    def _process_audio(self, file_path: str):
        """ğŸ”Š Process audio through transcription system"""
        self.transcription_results.clear()
        self.transcription_results.append("ğŸ¤ Transcribing audio with optimal model...")
        
        # In real implementation, this would:
        # 1. Send to hybrid smart loader with audio input
        # 2. Auto-select best audio model (Whisper variants)
        # 3. Get transcription results
        
        # Simulate processing delay
        QTimer.singleShot(3000, lambda: self._display_transcription_results(file_path))
    
    def _display_transcription_results(self, file_path: str):
        """ğŸ“ Display transcription results"""
        # Simulate transcription results
        results = f"""
ğŸ¤ Audio Transcription Complete

ğŸ“ File: {Path(file_path).name}
ğŸ¤– Model: Whisper-Large (auto-selected for audio transcription)
ğŸŒ Language: English (auto-detected)
ğŸ“Š Confidence: 96.2%

ğŸ“ Transcription:
[Simulated transcription text would appear here. The system automatically selected the optimal Whisper model variant based on audio properties and quality requirements.]

ğŸ“ˆ Analysis:
â€¢ Duration: 7.8 seconds
â€¢ Quality: High (16kHz sample rate)
â€¢ Background noise: Low
â€¢ Speaker count: 1

âš¡ Processing Time: 2.1 seconds
ğŸ§  Memory Used: 1.9 GB
        """
        
        self.transcription_results.clear()
        self.transcription_results.append(results)
        
        # Update performance
        self.performance_updated.emit({
            'memory_usage': 79,
            'gpu_usage': 68,
            'generation_speed': 52
        })
    
    def _send_chat_message(self):
        """ğŸ“¤ Send chat message with smart model integration"""
        message = self.chat_input.toPlainText().strip()
        if message:
            self.chat_history.append(f"ğŸ‘¤ You: {message}")
            self.chat_input.clear()
            
            # Use hybrid smart loader for response
            self._process_chat_message(message)
            
            self.log_message.emit(f"Chat message sent: {message}", "info")
    
    def _process_chat_message(self, message: str):
        """ğŸ§  Process chat message through smart model system"""
        # Simulate smart model processing
        self.chat_history.append("ğŸ¤– AI: Processing with optimal model...")
        
        # In real implementation, this would:
        # 1. Send to hybrid smart loader
        # 2. Auto-select best model for the input
        # 3. Get response and display
        
        # Simulate response delay
        QTimer.singleShot(1500, lambda: self._display_ai_response(message))
    
    def _display_ai_response(self, original_message: str):
        """ğŸ¤– Display AI response"""
        # Simulate smart response
        response = f"I understand you said: '{original_message}'. I selected the optimal model for this response based on your input type and current system resources."
        self.chat_history.append(f"ğŸ¤– AI: {response}")
        
        # Update performance metrics
        self.performance_updated.emit({
            'memory_usage': 82,
            'gpu_usage': 75,
            'generation_speed': 48
        })
    
    def _clear_chat(self):
        """ğŸ—‘ï¸ Clear chat history"""
        self.chat_history.clear()
        self.log_message.emit("Chat history cleared", "info")
    
    def handle_model_launch(self, config: dict):
        """ğŸš€ Handle model launch request from left panel"""
        model_name = config.get('model', 'Unknown')
        self.log_message.emit(f"Launching model: {model_name}", "info")
        
        # Update performance (placeholder)
        self.performance_updated.emit({
            'memory_usage': 85,
            'gpu_usage': 70,
            'generation_speed': 42
        })
    
    def handle_model_stop(self, model_id: str):
        """ğŸ›‘ Handle model stop request"""
        self.log_message.emit(f"Stopping model: {model_id}", "warning")
    
    def _discover_models(self):
        """ğŸ” Discover models in background"""
        try:
            self.log_message.emit("ğŸ” Starting model discovery...", "info")
            
            # Discover models (this runs synchronously for now)
            self.discovered_models = self.model_discovery.discover_models(force_rescan=False)
            
            # Log discovery results
            model_count = len(self.discovered_models)
            self.log_message.emit(f"ğŸ“Š Found {model_count} models", "info")
            
            # Update the model browser
            self._populate_model_browser()
            
            # Log final results
            stats = self.model_discovery.get_model_stats()
            self.log_message.emit(
                f"âœ… Discovery complete: Found {stats['total_models']} models "
                f"({stats['total_size_gb']:.1f} GB total)", 
                "success"
            )
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ Model discovery failed: {str(e)}"
            self.log_message.emit(error_msg, "error")
            print(f"Discovery Error: {e}")
            print(traceback.format_exc())
            self.discovered_models = {}
            self._populate_model_browser()
    
    def refresh_models(self):
        """ğŸ”„ Refresh model discovery"""
        self.log_message.emit("ğŸ”„ Refreshing model discovery...", "info")
        self.discovered_models = {}
        self._populate_model_browser()  # Show loading state
        self._discover_models()
    
    def add_model_directory(self, directory: str):
        """ğŸ“‚ Add new model directory"""
        if self.model_discovery.add_scan_directory_interactive(directory):
            self.refresh_models()
            return True
        return False
    
    def _add_model_path(self):
        """ğŸ“ Browse and add new model directory"""
        directory = QFileDialog.getExistingDirectory(
            self, "Select Model Directory", "", QFileDialog.ShowDirsOnly
        )
        if directory:
            if self.add_model_directory(directory):
                self.log_message.emit(f"âœ… Added model directory: {directory}", "success")
                self._update_paths_display()
            else:
                self.log_message.emit(f"âŒ Failed to add directory: {directory}", "error")
    
    def _open_config_file(self):
        """âš™ï¸ Open configuration file in default editor"""
        try:
            import subprocess
            import platform
            
            config_path = Path("./config/uml_config.json").resolve()
            
            if platform.system() == "Windows":
                subprocess.run(["notepad.exe", str(config_path)], check=False)
            elif platform.system() == "Darwin":  # macOS
                subprocess.run(["open", str(config_path)], check=False)
            else:  # Linux
                subprocess.run(["xdg-open", str(config_path)], check=False)
            
            self.log_message.emit(f"ğŸ“ Opened config file: {config_path}", "info")
        except Exception as e:
            self.log_message.emit(f"âŒ Failed to open config file: {str(e)}", "error")
    
    def _update_paths_display(self):
        """ğŸ“‚ Update the paths display with current configuration"""
        try:
            scan_dirs = self.model_discovery.config.get_expanded_scan_directories()
            extensions = self.model_discovery.config.get_file_extensions()
            
            paths_text = "ğŸ“‚ Current Scan Directories:\n"
            for i, path in enumerate(scan_dirs, 1):
                paths_text += f"  {i}. {path}\n"
            
            paths_text += f"\nğŸ“„ File Extensions: {', '.join(extensions)}\n"
            paths_text += f"\nğŸ“Š Models Found: {len(getattr(self, 'discovered_models', {}))}"
            
            self.paths_display.setText(paths_text)
        except Exception as e:
            self.paths_display.setText(f"âŒ Error loading configuration: {str(e)}")
    
    def _search_models(self):
        """ğŸ” Search discovered models"""
        if hasattr(self, 'search_input'):
            query = self.search_input.toPlainText().strip()
            if query:
                results = self.model_discovery.search_models(query)
                self.log_message.emit(f"ğŸ” Search '{query}': Found {len(results)} models", "info")
                # TODO: Update display with filtered results
            else:
                self.log_message.emit("âš ï¸ Please enter a search query", "warning")
